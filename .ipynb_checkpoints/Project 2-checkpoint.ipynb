{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of Dataset into Dataframe\n",
    "Load the dataset into a DataFrame. Perform necessary operations to properly load your dataset into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      atmpt1  atmpt2  atmpt3  atmpt4  atmpt5  atmpt6  atmpt7  atmpt8\n",
       "0       5.0    14.0     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "1       0.0     2.0    10.0    12.0     NaN     NaN     NaN     NaN\n",
       "2       3.0     5.0    13.0    14.0    16.0    17.0    19.0     NaN\n",
       "3       1.0     3.0     4.0     7.0     9.0    10.0    14.0    17.0\n",
       "4      15.0    16.0    17.0     NaN     NaN     NaN     NaN     NaN\n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...\n",
       "295     8.0    13.0    14.0     NaN     NaN     NaN     NaN     NaN\n",
       "296     4.0     6.0     9.0    11.0     NaN     NaN     NaN     NaN\n",
       "297     2.0     4.0     8.0    11.0    12.0     NaN     NaN     NaN\n",
       "298     5.0     6.0     7.0     9.0    13.0    14.0    16.0     NaN\n",
       "299     5.0     6.0     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "\n",
       "[300 rows x 8 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = [\"atmpt1\", \"atmpt2\", \"atmpt3\", \"atmpt4\", \"atmpt5\", \"atmpt6\", \"atmpt7\", \"atmpt8\"]\n",
    "df = pd.read_csv('Dataset1.csv', names=col_names,dtype=np.float64)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Description of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we chose is named \"Dataset1\" which contains 300 observations containing a series of numbers sorted in ascending order. The amount of numbers per row varies between observations and is not consistent within the dataset. Looking at the initial structure of the dataset afetr turning into a Datframe, we can safely assume that it is an Associate Rule Mining dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['atmpt1', 'atmpt2', 'atmpt3', 'atmpt4', 'atmpt5', 'atmpt6', 'atmpt7',\n",
       "       'atmpt8'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   atmpt1  300 non-null    float64\n",
      " 1   atmpt2  267 non-null    float64\n",
      " 2   atmpt3  222 non-null    float64\n",
      " 3   atmpt4  189 non-null    float64\n",
      " 4   atmpt5  151 non-null    float64\n",
      " 5   atmpt6  109 non-null    float64\n",
      " 6   atmpt7  80 non-null     float64\n",
      " 7   atmpt8  40 non-null     float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 18.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the observations in the dataset file. \n",
    "What does each observation represent? Since the dataset description is not provided, the group can presume the entity represented by each observation. For example:\n",
    "- For an association rule mining dataset, the group may presume that an observation represents a customer transaction from a store, a list of words in a document, and other similar examples.\n",
    "- For a clustering dataset, the group may presume that an observation represents a song from a group of songs, an image from a group of images, and other similar examples.\n",
    "- For a collaborative filtering dataset, the group may presume that an observation represents a movie being rated by people, a book being rated by readers, and other similar examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset that will be used for this Project is named \"Dataset1\" which contains 300 observations each containing a series of numbers which, based on our internal discussion, we will represent as a student's set of scores for a retakeable online quiz. As the dataset is an **association rule mining** dataset, we chose to represent the Dataset as such because its data fits the description of our representation quite well and is very relevant to our current situation during the pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the variables in the dataset file. \n",
    "Since the description of the dataset is not provided, the group can presume the entity represented by each variable. For example:\n",
    "- For an association rule mining dataset, the group may presume that a variable represents the presence of a certain item in a customer transaction from a store, the presence of a word in a document, and other similar examples.\n",
    "- For a clustering dataset, the group may presume that a variable represents a certain characteristic or feature of a song (i.e., value representing the tempo, rhythm, pitch, and others), a certain characteristic or feature of an image (i.e., amount of black, amount of white, and others), and other similar examples.\n",
    "- For a collaborative filtering dataset, the group may presume that a variable represents a rating of a specific person to a movie,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 variables in the dataset our group has chosen and they represent attempts made by the student to answer the online quiz. This is why we chose to name every column (as seen below) after each of the students' attempt in taking the online quiz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['atmpt1', 'atmpt2', 'atmpt3', 'atmpt4', 'atmpt5', 'atmpt6', 'atmpt7',\n",
       "       'atmpt8'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Perform exploratory data analysis comprehensively to gain a good understanding of your\n",
    "dataset. The exploratory data analysis should guide you in formulating the research\n",
    "questions of the project.\n",
    "\n",
    "In this section of the notebook, you must fulfill the following:\n",
    "• Identify 2 interesting exploratory data analysis questions. Properly state the\n",
    "questions in the notebook.\n",
    "\n",
    "• Answer the EDA questions using both:\n",
    "    o Numerical Summaries – measures of central tendency, measures of\n",
    "    dispersion, and correlation\n",
    "    \n",
    "    o Visualization – Appropriate visualization should be used. Each visualization should be accompanied by a brief explanation.\n",
    "\n",
    "• To emphasize, both numerical summary and visualization should be present to answer each question. The whole process should be supported with verbose textual descriptions of your procedures and findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "\n",
    "Identify the correct data mining technique to apply to your chosen dataset. The technique that you will apply should be appropriate for the dataset. Apply the data mining technique with the provided hyperparameters and answer the provided questions.\n",
    "\n",
    "For Association Rule Mining:\n",
    "- Use the rule_miner.py file from our exercises. Make sure that your code is working properly. Set support_t to 10 and the confidence_t to 0.6.\n",
    "- Perform association rule mining.\n",
    "- Answer the question: Using the provided support threshold and confidence threshold, what is/are the association rules that we derived from the dataset?\n",
    "\n",
    "For Clustering:\n",
    "- State the number of observations per group before clustering.\n",
    "- Use the kmeans.py file from our exercises. Make sure that your code is working properly. Set the k, start_var, end_var, num_observations, and data to their appropriate values according to the dataset.\n",
    "- Perform clustering with maximum iterations set to 300.\n",
    "- Answer the question: After clustering, how many observations of each class are included in per cluster?\n",
    "\n",
    "For Collaborative Filtering:\n",
    "- Use the collaborative_filtering.py file from our exercises. Make sure that your code is working properly. Set k to 5.\n",
    "- Perform collaborative filtering.\n",
    "- Answer the question: Give the top 5 items that are most similar to the item at index 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `RuleMiner` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule_miner import RuleMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `support_t` equal to `10` and `confidence_t` equal to `0.6`. The field `support_t` represents the support threshold, while the field `confidence_t` represents the confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_miner = RuleMiner(10, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['atmpt1'], ['atmpt2']]\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = rule_miner.get_frequent_itemsets(df)\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequent itemsets in the dataset, given the support threshold 10, is the set: `[['atmpt1'], ['atmpt2']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `get_rules()` function in `rule_miner.py`, let us list all the possible rules for all frequent itemsets in our dataset. The `get_rules()` function returns a list of rules produced from an itemset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[], ['atmpt1']], [['atmpt1'], []]]\n",
      "[[[], ['atmpt2']], [['atmpt2'], []]]\n"
     ]
    }
   ],
   "source": [
    "for itemset in frequent_itemsets:\n",
    "    print(rule_miner.get_rules(itemset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights and Conclusions\n",
    "Clearly state your answers from the data to answer each provided question. Make sure that all conclusions are backed up with proper data mining procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
